hydra:
  job:
    chdir: false
output_file: ???                 # refer to `bm25_retriever.yaml` for explanation
task_name: ???

batch_size: 64                   # the batch size when running encoding
model_name: 'bert-base-uncased'  # model used to encode 'field' for each index instance
faiss_index: ???                 # if file exists, the encoded index data will be directly loaded
pretrained_model_path: null      # the local pretrained encoder to load, load `model_name` if is null

num_ice: 50                      # the number of in-context examples

# parameters needed to initialize the input dataset
dataset_reader:
  _target_: src.dataset_readers.base_dsr.BaseDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: q
  dataset_split: validation
  dataset_path: null
  ds_size: null

# parameters needed to initialize the index_reader
index_reader:
  _target_: src.dataset_readers.index_dsr.IndexDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: q
  dataset_split: train
  dataset_path: null
  ds_size: null

# parameters needed to initialize the attention model
model_config:
  _target_: src.models.attention_model.AttentionModelConfig
  q_model_name: ${model_name}   # the encoder to encode input examples
  ctx_model_name: ${model_name} # the encoder to encode in-context examples
  q_no_grad: true               # whether to freeze the question encoder
  ctx_no_grad: true             # whether to freeze the context (in-context example) encoder, we share the two encoders for EPR and fix the ctx encoder for CEIL
  norm_embed: false             # whether to normalize the embedding after q and ctx encoder
  nhead: 8
  num_layers: 2
  max_num_ice: ${num_ice}
  nonlinear: true
  causal_mask: false
  class_ld: 1.
  score_ld: 1.