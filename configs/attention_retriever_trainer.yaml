hydra:
  job:
    chdir: false
task_name: ???
model_name: 'bert-base-uncased'
pretrained_model_path: null     # the pretrained local model to load (for CEIL, EPR model is used as initialization)
num_ice: 50                     # the number of in-context examples

# parameters needed to initialize the training arguments, see https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/trainer#transformers.TrainingArguments
training_args:
  _target_: transformers.TrainingArguments
  run_name: ???
  output_dir: ???
  do_train: true
  do_eval: true
  learning_rate: 0.001
  warmup_steps: 150
  num_train_epochs: 1000
  per_device_train_batch_size: 256
  per_device_eval_batch_size: 256
  gradient_accumulation_steps: 1
  evaluation_strategy: steps
  eval_steps: 50
  logging_steps: 10
  save_total_limit: 1
  save_strategy: steps
  save_steps: 50
  metric_for_best_model: eval_accuracy
  greater_is_better: true
  load_best_model_at_end: true

# parameters needed to initialize the input dataset
dataset_reader:
  _target_: src.dataset_readers.attention_retriever_training_dsr.AttentionRetrieverTrainingDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: q
  dataset_path: ???
  ds_size: null

# parameters needed to initialize the index reader
index_reader:
  _target_: src.dataset_readers.index_dsr.IndexDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: q
  dataset_split: train           # one of `dataset_path` and `dataset_split` must be set
  dataset_path: null
  ds_size: null

# parameters needed to initialize the batch collector
collector:
  num_ice: ${num_ice}           # the number of in-context examples

# parameters needed to initialize the attention model
model_config:
  _target_: src.models.attention_model.AttentionModelConfig
  q_model_name: ${model_name}   # the encoder to encode input examples
  ctx_model_name: ${model_name} # the encoder to encode in-context examples
  q_no_grad: true               # whether to freeze the question encoder
  ctx_no_grad: true             # whether to freeze the context (in-context example) encoder, we share the two encoders for EPR and fix the ctx encoder for CEIL
  norm_embed: false             # whether to normalize the embedding after q and ctx encoder
  nhead: 8
  num_layers: 2
  max_num_ice: ${num_ice}
  nonlinear: true
  causal_mask: false
  class_ld: 1.
  score_ld: 1.